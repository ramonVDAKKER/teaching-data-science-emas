{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramonVDAKKER/teaching-data-science-emas/blob/main/notebooks/intro_pca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIOxkmKV4D7h"
      },
      "source": [
        "# PRINCIPAL COMPONENT ANALYSIS\n",
        "\n",
        "The notebook contains illustrations and exercises corresponding to the topic <i>principal component analysis</i>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDdKvmio4D7o"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "064UhooL4D7q"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "%pylab inline\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from sklearn.decomposition import PCA\n",
        "from numpy.random import RandomState\n",
        "from numpy import linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpSZTbUS4D7r"
      },
      "source": [
        "# I. Illustration PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P2jEcSb4D7s"
      },
      "source": [
        "## PCA illustration 2-d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3v0tT3R4D7s"
      },
      "source": [
        "### Generate data $X_1,\\dots,X_n$ from bivariate Gaussian distribution and generate scatter plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F0AVKwe4D7s"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n = 200\n",
        "mean = [1, 3]\n",
        "rho = 0.75\n",
        "sigma_1 = 1\n",
        "sigma_2 = np.sqrt(1)\n",
        "cov = rho * sigma_1 * sigma_2\n",
        "covMatrix = [[sigma_1 ** 2, cov], [cov, sigma_2 ** 2]]\n",
        "# Simulate data:\n",
        "X = np.random.multivariate_normal(mean, covMatrix, n)\n",
        "# Scatter plot:\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal')\n",
        "plt.xlabel('$X_1$')\n",
        "plt.ylabel('$X_2$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUfeh0184D7t"
      },
      "source": [
        "### Determine principal components\n",
        "\n",
        "Note: as long as $n\\geq 2$ we can identify 2 principal components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LobKE4xC4D7u"
      },
      "source": [
        "Estimate covariance matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eBM08864D7u"
      },
      "outputs": [],
      "source": [
        "hatSigma = np.cov(X, rowvar=False)\n",
        "print(hatSigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URm_UCMG4D7v"
      },
      "source": [
        "Determine spectral decomposition using linear algebra package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LerYuRPB4D7v"
      },
      "outputs": [],
      "source": [
        "eigenvalues, matrixEigenvectors = LA.eig(hatSigma)\n",
        "print(\"Eigenvalues:\")\n",
        "print(eigenvalues)\n",
        "print(\"Matrix with normalized eigenvectors (in columns):\")\n",
        "print(matrixEigenvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4ogkCc54D7v"
      },
      "source": [
        "Warning: it is not guaranteed that LA.eig yields <i>ordered</i> eigenvalues.\n",
        "\n",
        "Check orthogonality eigenvectors and unit lengths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-o-9jcs4D7v"
      },
      "outputs": [],
      "source": [
        "np.dot(matrixEigenvectors.T, matrixEigenvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUSf7bPR4D7w"
      },
      "source": [
        "##### Question:\n",
        "- Determine the principal components.\n",
        "- Determine the % of variance explained by first component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjV3X_kA4D7w"
      },
      "outputs": [],
      "source": [
        "100 * eigenvalues[1] / (eigenvalues[0] + eigenvalues[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIDxOxtV4D7w"
      },
      "source": [
        "The actual/true principal components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IYOdHbV4D7w"
      },
      "outputs": [],
      "source": [
        "eigenvalues, matrixEigenvectors = LA.eig(covMatrix)\n",
        "print(\"Eigenvalues:\")\n",
        "print(eigenvalues)\n",
        "print(\"Matrix with normalized eigenvectors (in columns):\")\n",
        "print(matrixEigenvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXSAvlCt4D7w"
      },
      "source": [
        "The <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">pca module in the Scikit package</a> can do the work for us:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIh5oPP54D7w"
      },
      "outputs": [],
      "source": [
        "Xmean = X.mean(axis=0)\n",
        "Xdemean = X - Xmean # recall that data needs to be de-meaned\n",
        "n_princ_comp = 2\n",
        "pca = PCA(n_components=n_princ_comp, svd_solver='full')\n",
        "pca.fit(Xdemean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGQ1lobt4D7x"
      },
      "outputs": [],
      "source": [
        "components = pca.components_\n",
        "print(\"Principal components using SciKit package:\")\n",
        "print(components)\n",
        "eigenvalues2 = np.power(pca.singular_values_, 2) / (len(X) - 1) # The SciKit package computes singular value of Xdemeaned\n",
        "print(\"Eigenvalues using SciKit package:\")\n",
        "print(eigenvalues2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlDYSS1T4D7x"
      },
      "source": [
        "Note that Scikit organizes principal components in rows!  (For $d=p$ confusion could arise. So always try $d<p$ as well.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QtqR0jv4D7x"
      },
      "outputs": [],
      "source": [
        "vr = pca.explained_variance_ratio_  # \\lambda_j / \\sum_{j=1}^p \\lambda_j  for j=1,...,d\n",
        "y_pos = np.arange(len(vr))\n",
        "plt.bar(y_pos, vr)\n",
        "plt.bar(y_pos, 100 * vr, align='center', alpha=0.5)\n",
        "plt.ylabel('%')\n",
        "plt.xlabel('no. principal component')\n",
        "plt.title('% of variance explained by x-th principal component')\n",
        "plt.show()\n",
        "print(\"The first %d principal components explain %d percent of the variance\" % (len(vr), 100*sum(vr)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRpqP4QB4D7x"
      },
      "source": [
        "### Plot the principal components using the corresponding eigenvalue (which equals $\\operatorname{var}\\left(  w_j^\\prime X\\right)$ ) as length of the vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjtuTWiL4D7x"
      },
      "outputs": [],
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->',\n",
        "                    linewidth=2,color='b',\n",
        "                    shrinkA=0, shrinkB=0)\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 2 * np.sqrt(length)\n",
        "    draw_vector(Xmean, Xmean + v)\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okqb_ZiG4D7x"
      },
      "source": [
        "### Question:\n",
        "Repeat the above for $\\rho=0.95$ and $\\rho=0$ and for $n=30$ and $n=10000$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikAnJU7Y4D7y"
      },
      "source": [
        "# II. Application PCA as dimension reduction/ data compression technique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zl1jLW04D7y"
      },
      "source": [
        "<a href=\"https://www.cs.ucsb.edu/~mturk/Papers/jcn.pdf\">Turk and Pentland (1991)</a> is (one of the) first paper(s) to use PCA in the context of face recognition. See <a href=\"http://www.face-rec.org/\">link</a> for several datasets and (modern) algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sFlnG4V4D7y"
      },
      "source": [
        "### Retrieve  <a href=\"http://scikit-learn.org/stable/datasets/olivetti_faces.html\">Olivetti faces dataset</a>\n",
        "\n",
        "   \n",
        "\n",
        "\"The dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. As described on the original website:\n",
        "\n",
        "There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n",
        "The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.\n",
        "\n",
        "The “target” for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.\n",
        "\n",
        "The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OehJHrP74D7y"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces # dataset\n",
        "dataset = fetch_olivetti_faces(shuffle=True, random_state=RandomState()) # retrieve data\n",
        "faces = dataset.data # array containing data, columns=features correspond to the 64 x 64 pixels\n",
        "n_samples, n_features = faces.shape\n",
        "print(\"The dataset consists of %d pictures\" % n_samples)\n",
        "print(\"Each picture contains %d features which corresponds to 64x64 (pixels)\" % n_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6myrmY8I4D7y"
      },
      "source": [
        "### Plot a few faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlQRTQrW4D7y"
      },
      "outputs": [],
      "source": [
        "# function to plot faces\n",
        "def plot_gallery(title, images, image_shape, n_col, n_row):\n",
        "    plt.figure(figsize=(3. * n_col, 3.26 * n_row))\n",
        "    plt.suptitle(title, size=16)\n",
        "    for i, comp in enumerate(images):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        vmax = max(comp.max(), -comp.min())\n",
        "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
        "                   interpolation='nearest',\n",
        "                   vmin=-vmax, vmax=vmax)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
        "plot_gallery(\"True faces\", faces[10:15,:], (64,64), 5,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHBCD1IN4D7y"
      },
      "source": [
        "### Center data to prepare for PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye2zQN8o4D7y"
      },
      "outputs": [],
      "source": [
        "averageFace = faces.mean(axis=0)\n",
        "faces_centered = faces - averageFace\n",
        "plot_gallery(\"Average face\", averageFace.reshape(n_features,1).T,(64,64),5,1)\n",
        "plot_gallery(\"True faces\", faces[10:15,:],(64,64),5,1)\n",
        "plot_gallery(\"Centered faces\", faces_centered[10:15,:],(64,64),5,1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQZ8HQ674D7z"
      },
      "source": [
        "### Compute PCA and check % variance explained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2OLa2Nq4D7z"
      },
      "outputs": [],
      "source": [
        "n_princ_comp = 50  # here you can adapt the no. of principal components [max=400; why?],  please use >=5!!!\n",
        "#\n",
        "pca = PCA(n_components=n_princ_comp, svd_solver='full')\n",
        "pca.fit(faces_centered)\n",
        "vr = pca.explained_variance_ratio_  # \\lambda_j / \\sum_{j=1}^p \\lambda_j  for j=1,...,d\n",
        "y_pos = np.arange(len(vr))\n",
        "plt.bar(y_pos, vr)\n",
        "plt.bar(y_pos, 100*vr, align=\"center\", alpha=0.5)\n",
        "plt.ylabel(\"%\")\n",
        "plt.xlabel(\"no. principal component\")\n",
        "plt.title(\"% of variance explained by x-th principal component\")\n",
        "plt.show()\n",
        "print(\"The first %d principal components explain %d percent of the variance\" % (len(vr), 100*sum(vr)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ephhOsyd4D7z"
      },
      "outputs": [],
      "source": [
        "components = pca.components_\n",
        "sv = pca.singular_values_\n",
        "print(\"The average face:\")\n",
        "plot_gallery(\"Average face\", averageFace.reshape(n_features,1).T, (64, 64), 10, 1)\n",
        "plt.show()\n",
        "print(\"The 5 faces, called eigen faces, corresponding to the first 10 principal components:\")\n",
        "plot_gallery(\"Eigen faces\", components[0:10, :],(64, 64), 10, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftEFFkT44D7z"
      },
      "source": [
        "### Let us construct a new face by taking linear combination of eigen faces (weights=1) and adding average face:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFstD_EL4D7z"
      },
      "outputs": [],
      "source": [
        "newFace =  np.sum(components[0:10,:], axis=0)\n",
        "plot_gallery(\"New face\", newFace.reshape(n_features,1).T, (64, 64), 10, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGylSeC14D7z"
      },
      "source": [
        "### Reconstruct faces in dataset [on basis of average face and the eigen faces]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-U8B0jw4D7z"
      },
      "outputs": [],
      "source": [
        "scores = pca.transform(faces_centered)\n",
        "reconstructedFaces = pca.inverse_transform(scores) + averageFace\n",
        "plot_gallery(\"True faces\", faces[10:15,:], (64, 64), 10, 1)\n",
        "plot_gallery(\"Reconstructed Faces based on PCA using d=\" + str(n_princ_comp), reconstructedFaces[10:15,:], (64, 64), 10, 1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBeaLoM44D7z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}