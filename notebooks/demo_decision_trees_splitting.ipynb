{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramonVDAKKER/teaching-data-science-emas/blob/main/notebooks/demo_decision_trees_splitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8khFW1eoeAFH"
      },
      "source": [
        "# The basics of decision trees\n",
        "\n",
        "This notebook contains snippets to facilitate a demo and discussion during a lecture. We focus on the technique and not on understanding the empirical application associated to the dataset we use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrVdEnVveDjn"
      },
      "source": [
        "## 0. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CJCn2cadNeF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import plot_partial_dependence\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RV02kMLhRne"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "!{sys.executable} -m pip install -U pandas-profiling[notebook]\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "import pandas_profiling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "8WjnvOJJr33L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlbhxSw0ealm"
      },
      "source": [
        "## 1. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_4q5HhSd7oX"
      },
      "source": [
        "data = load_breast_cancer()\n",
        "y_df = pd.DataFrame(data.target, columns=[\"target\"])\n",
        "X_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=0.3, random_state=123)\n",
        "data_train_df = pd.concat([y_train_df, X_train_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjZ8dXCAd7uL"
      },
      "source": [
        "print(f\"The dataset, available for training, has {len(data_train_df)} rows and {data_train_df.shape[1]} columns.\")\n",
        "print(\"Some rows of the dataset:\")\n",
        "display(data_train_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsEMo2g-o5pT"
      },
      "source": [
        "## 2. Splitting mechanism illustrated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOQyNULsp6z-"
      },
      "source": [
        "def gini_impurity(y_left, y_right):\n",
        "  p_L = np.mean(y_left.values)\n",
        "  gini_left = 2 * p_L * (1 - p_L)\n",
        "  p_R = np.mean(y_right.values)\n",
        "  gini_right = 2 * p_R * (1 - p_R)\n",
        "  n_right = len(y_right)\n",
        "  weight_right = n_right / (len(y_left) + n_right)\n",
        "  gini = weight_right * gini_right + (1 - weight_right) * gini_left\n",
        "  return gini, gini_left, gini_right\n",
        "def determine_gini_impurity_for_feature_and_split_ext(y, X_df, name_feature, threshold):\n",
        "  left = (X_df[name_feature] <= threshold)\n",
        "  y_left = y[left]\n",
        "  y_right = y[~ left]\n",
        "  return gini_impurity(y_left, y_right) \n",
        "def determine_gini_impurity_for_feature_and_split(y, X_df, name_feature, threshold):\n",
        "  gini, _, _ = determine_gini_impurity_for_feature_and_split_ext(y, X_df, name_feature, threshold)\n",
        "  return gini\n",
        "vector_determine_gini_impurity_for_feature_and_split = np.vectorize(determine_gini_impurity_for_feature_and_split, excluded=[\"y\", \"X_df\"])\n",
        "def analyze_feature(X_df, name_feature, y):\n",
        "  feature_values = X_df[name_feature].sort_values(ascending=True)[1:-1]\n",
        "  gini = vector_determine_gini_impurity_for_feature_and_split(y=y, X_df=X_df, name_feature=name_feature, threshold=feature_values)\n",
        "  plt.plot(feature_values, gini)\n",
        "  plt.ylabel(\"Gini\")\n",
        "  plt.xlabel(\"Splitvalue\")\n",
        "  plt.title(f\"Gini as function of possible split values for feature {name_feature}.\")\n",
        "  print(f\"Split value: {feature_values.iloc[gini.argmin()]}\")\n",
        "  print(f\"Gini: {gini.min()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPHybiL0Dhjq"
      },
      "source": [
        "Let us analyze a feature and evaluate how we should split for this feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUwfRrytEA7S"
      },
      "source": [
        "name_feature = \"mean perimeter\"\n",
        "sns.histplot(data=data_train_df, x=name_feature, hue=\"target\", multiple=\"stack\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ5r9N6fF-SM"
      },
      "source": [
        "The following graph shows how the Gini varies with the threshold:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j4a1yZMrUP3"
      },
      "source": [
        "analyze_feature(X_train_df, name_feature, y_train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6bov-souq8f"
      },
      "source": [
        "def determine_best_split(y, X_df):\n",
        "  out_df = pd.DataFrame(columns=[\"Gini\", \"Split\"])\n",
        "  for name_feature in X_df.columns:\n",
        "    feature_values = X_df[name_feature].sort_values(ascending=True)[1:-1]\n",
        "    gini = vector_determine_gini_impurity_for_feature_and_split(y=y, X_df=X_df, name_feature=name_feature, threshold=feature_values)\n",
        "    out_df.loc[name_feature, \"Split\"] = feature_values.iloc[gini.argmin()]\n",
        "    out_df.loc[name_feature, \"Gini\"] = gini.min()\n",
        "  return out_df\n",
        "display(determine_best_split(y_train_df, X_train_df).sort_values(by=\"Gini\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov0s0LUzzj-t"
      },
      "source": [
        "## 4. Train decision tree using Scikit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlwQk0JIG7ck"
      },
      "source": [
        "Let us check that Scikit finds the same answer for the first split. To this end we train a decision tree with 1 as maximum depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V38OrS5wxcFp"
      },
      "source": [
        "dtree = tree.DecisionTreeClassifier(max_depth=1)\n",
        "dtree = dtree.fit(X_train_df, y_train_df)\n",
        "tree.plot_tree(dtree) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Wj_Np93GIm"
      },
      "source": [
        "name_feature = X_train_df.columns[20]\n",
        "sns.histplot(data=data_train_df, x=name_feature, hue=\"target\", multiple=\"stack\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_5Sh5pUHdUz"
      },
      "source": [
        "Let us check if our ad hoc code yields the same Gini for both leaves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvPb0bG99Jis"
      },
      "source": [
        "print(f\"Gini before splitting:  {(2 * np.mean(y_train_df.values) * (1 - np.mean(y_train_df.values))):.03f}\")\n",
        "split_value = 16.795\n",
        "gini, gini_left, gini_right = determine_gini_impurity_for_feature_and_split_ext(y_train_df, X_train_df, name_feature, split_value)\n",
        "print(f\"Gini left: {gini_left:.03f}.\")\n",
        "print(f\"Gini right: {gini_right:.03f}.\")\n",
        "frac_left = np.mean(X_train_df[name_feature] <= split_value)\n",
        "print(f\"Gini after splitting: {frac_left * gini_left + (1 - frac_left) * gini_right:.03f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHUzcgLvIZnM"
      },
      "source": [
        "Now we understand how the decision tree comes to its splits, we train a \"full\" tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2FvsDSe_rwS"
      },
      "source": [
        "dtree = tree.DecisionTreeClassifier()\n",
        "dtree = dtree.fit(X_train_df, y_train_df)\n",
        "tree.plot_tree(dtree) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZxS9YnjBWE-"
      },
      "source": [
        "Use the tree to predict $Y$ for an observation $X$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkx5eJKKBmpH"
      },
      "source": [
        "x = X_train_df.loc[0]\n",
        "print(\"For\\n\\n\")\n",
        "print(x)\n",
        "print(f\"\\n\\nthe tree yields Y_hat={dtree.predict(x.values.reshape(1, -1))[0]}.\")\n",
        "print(f\"The true outcome is {y_train_df.loc[0][0]}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2J1_UHBDJb4"
      },
      "source": [
        "Now we \"score\" full dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrnkOaSjAPJi"
      },
      "source": [
        "y_train_pred = dtree.predict(X_train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGrTKHteAYno"
      },
      "source": [
        "print(y_train_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezQj96hgDaVp"
      },
      "source": [
        "Let us determine the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWA9zwMhAadG"
      },
      "source": [
        "class_names = [0, 1]\n",
        "def confusion_matrix(model_object, X_df, y_df):\n",
        "    disp = plot_confusion_matrix(model_object, X_df, y_df)\n",
        "    disp.ax_.set_title(\"Confusion matrix\")\n",
        "    plt.show()\n",
        "    y_hat = model_object.predict(X_df)\n",
        "    accuracy = accuracy_score(y_df, y_hat, normalize=True)\n",
        "    print(f\"The accuracy is {100 * accuracy:0.2f}%.\")\n",
        "confusion_matrix(dtree, X_train_df, y_train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8uQG8y7DlCw"
      },
      "source": [
        "Next we evaluate the model on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf7YW21oAagO"
      },
      "source": [
        "confusion_matrix(dtree, X_test_df, y_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fRhnksiE59z"
      },
      "source": [
        "Let us also fit a decision tree with restricted depth and evaluate its performance on the train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrNAAL9REn3D"
      },
      "source": [
        "dtree = tree.DecisionTreeClassifier(max_depth=3)\n",
        "dtree = dtree.fit(X_train_df, y_train_df)\n",
        "confusion_matrix(dtree, X_train_df, y_train_df)\n",
        "confusion_matrix(dtree, X_test_df, y_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VoZoqzdpsMh1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}