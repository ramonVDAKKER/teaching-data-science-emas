{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":[]},{"cell_type":"markdown","metadata":{"id":"xYLR803nL1w1"},"source":["# Fraud detection\n","\n","In case of comments or questions: please contact <a href=\"mailto:R.vdnAkker@uvt.nl\">Ramon van den Akker</a>.\n","\n","The notebook contains illustrations and exercises corresponding to the module Data Science II."]},{"cell_type":"markdown","metadata":{"id":"7zkHU1ziL1w5"},"source":["# 0. Install additional packages (you only need to do this once)"]},{"cell_type":"markdown","metadata":{"id":"Qto42wtSL1w6"},"source":["We need an additional package, imbalanced-learn."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vbyy-cv8L1w6"},"outputs":[],"source":["# Install the package imblearn in the current Jupyter kernel\n","!pip install imbalanced-learn"]},{"cell_type":"markdown","metadata":{"id":"kGC-M7mpL1w8"},"source":["# 0. Import standard packages\n","\n","Important packages for Python are <a href=\"http://www.numpy.org/\">numpy</a> (for arrays, linear algebra, pseudorandom numbers etc.), <a href=\"http://pandas.pydata.org/\">pandas</a> (contains convenient data structure called \"pandas dataframe\"), <a href=\"http://matplotlib.org/\">matplotlib</a> & <a href=\"http://seaborn.pydata.org/\"> seaborn</a> (for data visualisation), <a href=\"http://scikit-learn.org/stable/\">sklearn</a> (scikit-learn; powerful package containing machine & statistical learning functions).\n","\n","Typically all import statements are organized at the top of the notebook. In case you get an error stating a package is missing you can open the <i>Anaconda prompt</i> and enter <i>conda install name-package</i>. In case this does not work you can resort to <i>pip install name-package</i> or <i>easy_install name-package</i>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwfHJ0zEL1w8"},"outputs":[],"source":["# Standard packages:\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib\n","from matplotlib import cm as cm\n","%matplotlib inline\n","import seaborn as sns\n","plt.style.use(\"seaborn-deep\")\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve\n","from sklearn.ensemble import IsolationForest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYoYfYrsL1w9"},"outputs":[],"source":["# Special packages"]},{"cell_type":"markdown","source":[],"metadata":{"id":"Ru9gJpxHshiw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTp6yNMNL1w-"},"outputs":[],"source":["from imblearn.over_sampling import RandomOverSampler, SMOTE"]},{"cell_type":"markdown","metadata":{"id":"3Is8kwDtL1w-"},"source":["# 1. Getting started"]},{"cell_type":"markdown","metadata":{"id":"wv2jlK1SL1w_"},"source":["## 1.1 Data retrieval\n","\n","The dataset we will use originates from a Kaggle competition; see <a href=\"https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3\">link</a>. The data is available in a csv-file.\n","\n","This file should be available in the same folder as this notebook."]},{"cell_type":"markdown","metadata":{"id":"D2Dw7vlPL1w_"},"source":["##### Load data into a pandas dataframe from provided csv-file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ps2C0In5L1w_"},"outputs":[],"source":["url_data = \"https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv\"\n","df = pd.read_csv(url_data, sep=\",\")"]},{"cell_type":"markdown","metadata":{"id":"BzboieKeL1xA"},"source":["##### Inspect first rows of dataframe and information on dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yS96FYjrL1xA"},"outputs":[],"source":["df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfDNlMiFL1xA"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUkpjvAIL1xA"},"outputs":[],"source":["print(f\"The number of observations is {df.shape[0]}\")\n","print(f\"The number of variables is {df.shape[1]}\")"]},{"cell_type":"markdown","metadata":{"id":"uuSqBv6HL1xB"},"source":["Provide list of labels corresponding to Class=0 and Class=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOJo9oOfL1xB"},"outputs":[],"source":["target_names = [\"non-fraud\", \"fraud\"]"]},{"cell_type":"markdown","metadata":{"id":"qnpsGsvpL1xB"},"source":["If performance turns out to be (too) slow: uncomment the following lines and run the cell to downsample the dataframe."]},{"cell_type":"markdown","metadata":{"id":"QeIMaJFAL1xC"},"source":["## 1.2 Generate train, validation, and test sets"]},{"cell_type":"markdown","metadata":{"id":"7okck8vhL1xC"},"source":["Construct a train, validation, and test set. And organize the features in dataframes X and the target in dataframes y."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmaLAjrtL1xC"},"outputs":[],"source":["seed = 140\n","X_train, X_aux, y_train, y_aux = train_test_split(df.drop(columns=[\"Class\"]), df[\"Class\"], test_size=0.5, random_state=seed)\n","X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.5)\n","#\n","print(f\"data_train shape: {X_train.shape}\")\n","print(f\"data_validation shape: {X_val.shape}\")\n","print(f\"data_test shape: {X_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"Zeg06xV4L1xC"},"source":["# 2. Elementary Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"Ph4ihWp5L1xC"},"source":["### Check the descriptive statistics:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxCYxZDgL1xC"},"outputs":[],"source":["X_train.describe()"]},{"cell_type":"markdown","metadata":{"id":"8vqyeOplL1xC"},"source":["### Inspect estimated correlation matrix:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTp6gIiML1xD"},"outputs":[],"source":["X_train.corr(method=\"pearson\")"]},{"cell_type":"markdown","metadata":{"id":"pBREHEJDL1xD"},"source":["### Perhaps easier to analyze estimated correlations via a visualization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gwnLmHEL1xD"},"outputs":[],"source":["def VizCorrelationMatrix(df):\n","    fig = plt.figure()\n","    ax1 = fig.add_subplot(111)\n","    cmap = cm.get_cmap(\"jet\", 30)\n","    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n","    ax1.grid(True)\n","    plt.title(\"Estimated Correlation Matrix\")\n","    fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n","    plt.show()\n","VizCorrelationMatrix(X_train)\n","print(pd.DataFrame(X_train.columns, columns=[\"Name feature\"]))"]},{"cell_type":"markdown","metadata":{"id":"Ys-S2cmTL1xD"},"source":["### Distribution of the target:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1kazaRzL1xD"},"outputs":[],"source":["print(\"Recall, from the description of the data, that target=1 corresponds to a fraud.\")\n","print(\"\\n\")\n","print(\"The data type of the target is \" + str(type(y_train)))\n","print(\"\\n\")\n","print(\"The distribution of the target in the train set:\")\n","unique, counts = np.unique(y_train, return_counts=True)\n","print(\" - value \" + str(unique[0]) + \": \" + str(counts[0]) + \" observations;\" )\n","print(\" - value \" + str(unique[1]) + \": \" + str(counts[1]) + \" observations.\")\n","print(\"\\n\")\n","print(\"The frequency of observations with Y=1 equals (in the train set): \" + str(np.round(100*counts[1]/(counts[0]+counts[1]),1)) + \"%.\")"]},{"cell_type":"markdown","metadata":{"id":"vUHie4guL1xD"},"source":["#### Question\n","Is this dataset imbalanced?"]},{"cell_type":"markdown","metadata":{"id":"TkgK_HnyL1xD"},"source":["### Check histograms features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gafiWNEBL1xD"},"outputs":[],"source":["X_train.hist(bins=50, figsize=(20,15))"]},{"cell_type":"markdown","metadata":{"id":"DsxSw9LHL1xE"},"source":["There are several packages available that provide an extensive Explorative Data Analysis. See, for example, https://github.com/pandas-profiling/pandas-profiling for the pandas-profiling package."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Ifx6YiHKL1xE"},"source":["# 3. Univariate predictive performance\n","The following histograms show the marginal distributions of the features in the groups $\\{Y=1\\}$ (True) and $\\{Y=0\\}$ (False).\n","\n","(Note that the histograms are scaled.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ti6AbC3L1xE"},"outputs":[],"source":["Z = X_train.copy()\n","Z[\"target\"] = pd.DataFrame(y_train)\n","for name in X_train.columns:\n","    print(\"Consider feature \" + name + \":\")\n","    x = Z.where(Z[\"target\"]==1)[name]\n","    y = Z.where(Z[\"target\"]==0)[name]\n","    left =min(np.nanmin(x), np.nanmin(y))\n","    right =max(np.nanmax(x), np.nanmax(y))\n","    plt.hist([x, y], bins=25, range=[left, right], label=['1', '0'], density=True)\n","    plt.legend(loc=\"upper right\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DOj8ddXuL1xE"},"source":["##### Question\n","Which features seem to be promising?"]},{"cell_type":"markdown","metadata":{"id":"nRgh87fTL1xE"},"source":["##### Question\n","What other analyses could you think of to assess the predictive performance of features?"]},{"cell_type":"markdown","metadata":{"id":"1SGYMfsRL1xE"},"source":["# 3. Elementary Model Exploration"]},{"cell_type":"markdown","metadata":{"id":"MxYLfV2hL1xE"},"source":["## 3.1  Decision Tree\n","\n","Check the documentation of Scikit, https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier,  for further details."]},{"cell_type":"markdown","metadata":{"id":"uKa61qteL1xE"},"source":["Initialization of learner (create object):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-wM1_WqL1xF"},"outputs":[],"source":["dt = tree.DecisionTreeClassifier(max_depth=10)"]},{"cell_type":"markdown","metadata":{"id":"GaHtudKkL1xF"},"source":["Estimate model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaG-ixEaL1xF"},"outputs":[],"source":["dt.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"RTStsPqBL1xF"},"source":["Determine classifications on train and validation sets using estimated model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qO6fLdUL1xF"},"outputs":[],"source":["hat_y_train_dt = dt.predict(X_train)\n","hat_y_val_dt = dt.predict(X_val)"]},{"cell_type":"markdown","metadata":{"id":"Pv4gaRCxL1xF"},"source":["Inspect type of output that <i>predict</i> yields:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFxz7waJL1xF"},"outputs":[],"source":["plt.hist(hat_y_train_dt)\n","print(f\"Unique values: {np.unique(hat_y_train_dt)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_X4Lv9BL1xF"},"outputs":[],"source":["# As we are going to plot a lot of confusion matrices, we create a function:\n","def plot_confusion_matrix(hat_y, y, target_names):\n","    matrix = confusion_matrix(y, hat_y)  # note that true label corresponds to first argument\n","    sns.heatmap(matrix.T, square=True, annot=True, fmt=\"d\", cbar=False,\n","    xticklabels=target_names, yticklabels=target_names)\n","    plt.xlabel(\"true label\")\n","    plt.ylabel(\"predicted label\")\n","    accuracy = accuracy_score(y, hat_y, normalize=True, sample_weight=None)\n","    print(\"The accuracy is \" + str(np.round(100*accuracy,1)) + \"%\")\n","plot_confusion_matrix(hat_y_train_dt, y_train, target_names)"]},{"cell_type":"markdown","metadata":{"id":"mGBiToWGL1xF"},"source":["##### Question\n","Check, using the command `sum(...)' that the matrix is indeed correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gh2ZJOr2L1xF"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6E_HV1XNL1xF"},"source":["##### Question\n","What is the number of False Positives?  "]},{"cell_type":"markdown","metadata":{"id":"HIWWEHCQL1xG"},"source":["##### Question  \n","Determine classifications on validation set using the estimated model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WimGUIaL1xG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yTASDmC1L1xG"},"source":["##### Question\n","The accuracy is almost perfect. How do you assess the quality of the model?"]},{"cell_type":"markdown","metadata":{"id":"Ykdx3fRIL1xG"},"source":["## 3.2 Random forest"]},{"cell_type":"markdown","metadata":{"id":"8Kf8ARQSL1xG"},"source":["You can reduce n_estimators to speed up calculations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzCFmIlnL1xG"},"outputs":[],"source":["clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=123)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgqesxwAL1xG"},"outputs":[],"source":["rf = clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGVsc9pNL1xG"},"outputs":[],"source":["hat_y_train_rf = rf.predict(X_train)\n","hat_y_val_rf = rf.predict(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfpGNgxaL1xG"},"outputs":[],"source":["print(\"Results for train set:\")\n","plot_confusion_matrix(hat_y_train_rf, y_train, target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltJq_A8aL1xG"},"outputs":[],"source":["print(\"Results for validation set:\")\n","plot_confusion_matrix(hat_y_val_rf, y_val, target_names)"]},{"cell_type":"markdown","metadata":{"id":"-WEZ-22EL1xG"},"source":["# 4. Cost-sensitive learning"]},{"cell_type":"markdown","metadata":{"id":"DwkVCXllL1xH"},"source":["Let us first inspect that our classifiers can also deliver a score / probability as output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljVDmjq6L1xH"},"outputs":[],"source":["rf.predict_proba(X_val)"]},{"cell_type":"markdown","metadata":{"id":"l5S2Ql-YL1xH"},"source":["You see that two columns are generated. The second one corresponds to (the probability of) the `fraud' class."]},{"cell_type":"markdown","metadata":{"id":"te_ShI5YL1xH"},"source":["### Cost misclassification\n","Let us use as misclassification costs:\n","- for True Negative and True Positive: 0\n","- for False Negative (missed fraud): 250\n","- for False Positive (false alert): 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbeJPUPfL1xH"},"outputs":[],"source":["cost_FP = 1\n","cost_FN = 250"]},{"cell_type":"markdown","metadata":{"id":"CQNIRE6tL1xH"},"source":["# 4.1. Optimal treshold for \"probability learners\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdySmbpsL1xH"},"outputs":[],"source":["optimal_treshold = (cost_FP - 0) / (cost_FP - 0 + cost_FN - 0)\n","print(\"For `probability leaners the optimal treshold (assuming we are dealing with true probabilities) is \" + str(np.round(100*optimal_treshold, 3)) + \"%\")"]},{"cell_type":"markdown","metadata":{"id":"Kbk9-nM3L1xH"},"source":["### Let us reconsider the estimated decision tree\n","\n","We will compare the standard decision tree to the \"optimal-treshold dt\"  which classifies an observation as \"1\" in case the estimated probability exceeds the threshold above."]},{"cell_type":"markdown","metadata":{"id":"Y2cea7y7L1xH"},"source":["Classifications decision tree using optimal_treshold:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wv9Kbn8AL1xH"},"outputs":[],"source":["hat_y_val_dt_ot = (dt.predict_proba(X_val)[:,1] > optimal_treshold)\n","hat_y_train_dt_ot = (dt.predict_proba(X_train)[:,1] > optimal_treshold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifvFQFgVL1xH"},"outputs":[],"source":["print(\"The confusion matrix on validation set for `standard' decision tree:\")\n","plot_confusion_matrix(hat_y_val_dt, y_val, target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lz06q-UAL1xI"},"outputs":[],"source":["print(\"The confusion matrix on validation set for `optimal-treshold' decision tree:\")\n","plot_confusion_matrix(hat_y_val_dt_ot, y_val, target_names)"]},{"cell_type":"markdown","metadata":{"id":"ZlZeIJXSL1xI"},"source":["### Question:\n","Also evaluate the costs on the train set."]},{"cell_type":"markdown","metadata":{"id":"0EuLxjYqL1xI"},"source":["### Question:\n","Are the results as expected?"]},{"cell_type":"markdown","metadata":{"id":"NyvDoxgRL1xI"},"source":["Let us evaluate costs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfnwAvfZL1xI"},"outputs":[],"source":["def estimate_cost(hat_y, y, cost_FP, cost_FN):\n","    return  np.sum(np.multiply(hat_y, (1 - y)) * cost_FP) + np.sum(np.multiply((1 - hat_y), y) * cost_FN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfbRGsgyL1xI"},"outputs":[],"source":["c_dt_st = estimate_cost(hat_y_val_dt, y_val, cost_FP, cost_FN)\n","c_dt_ot = estimate_cost(hat_y_val_dt_ot, y_val, cost_FP, cost_FN)\n","print(\"Cost misclassification using `standard' decision tree: \"\n","          +  str(np.int(c_dt_st) ))\n","print(\"Cost misclassification using `optimal-treshold' decision tree: \"\n","          +  str(np.int(c_dt_ot)) )\n","print(\"Ratio (standard/o-treshold): \" + str(np.round(100 * c_dt_st / c_dt_ot, 1)) + \"%\")"]},{"cell_type":"markdown","metadata":{"id":"EOiRJC7JL1xI"},"source":["##### Question\n","Analyze the performance of the estimated random forest in combination with the `optimal-treshold'."]},{"cell_type":"markdown","metadata":{"id":"zBSUQq45L1xI"},"source":["## 4.2. Cost-sensitive decision tree"]},{"cell_type":"markdown","metadata":{"id":"Rb3cK7v-L1xI"},"source":["The decision tree of Scikit is able to accept class-weights as input."]},{"cell_type":"markdown","metadata":{"id":"18Jg6SE2L1xI"},"source":["##### Question\n","Check, using the cells below, that such class-weights indeed have an impact on the <i>internal</i> structure of the tree (different splits and/or different selection of features)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uf5Lx9TGL1xI"},"outputs":[],"source":["# auxiliary function\n","def class_weight(cost_FP, cost_FN):\n","    return {0: cost_FP, 1: cost_FN}\n","# standard decision tree\n","ctree = tree.DecisionTreeClassifier(max_depth=2)\n","tree.plot_tree(ctree.fit(X_train, y_train.values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itbzLK1tL1xJ"},"outputs":[],"source":["# cost-sensitive decision tree\n","ctree = tree.DecisionTreeClassifier(max_depth=2, class_weight=class_weight(cost_FP, cost_FN))\n","tree.plot_tree(ctree.fit(X_train, y_train.values))"]},{"cell_type":"markdown","metadata":{"id":"m3MLnG2HL1xJ"},"source":["Next we fit a cost-sensitive decision tree and compare the resulting performance to that of a standard decision tree (with the same depth)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lb80UFHhL1xJ"},"outputs":[],"source":["ctree = tree.DecisionTreeClassifier(max_depth=10, class_weight=class_weight(cost_FP, cost_FN))\n","ctree.fit(X_train, y_train.values)\n","hat_y_val_ctree = ctree.predict(X_val)\n","hat_y_train_ctree = ctree.predict(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LUrM4wxL1xJ"},"outputs":[],"source":["print(\"Recall the confusion matrix on train set for `standard' decision tree:\")\n","plot_confusion_matrix(hat_y_train_dt, y_train, target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rldNPA0UL1xJ"},"outputs":[],"source":["print(\"The confusion matrix for cost-sensitive decision tree:\")\n","plot_confusion_matrix(hat_y_train_ctree, y_train, target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNqaQfPGL1xJ"},"outputs":[],"source":["cost_DT_ot = estimate_cost(hat_y_train_dt_ot, y_train, cost_FP, cost_FN)\n","cost_CSDT = estimate_cost(hat_y_train_ctree, y_train, cost_FP, cost_FN)\n","print(\"Cost misclassification using `standard' decision tree with optimal-treshold: \"\n","          +  str(cost_DT_ot) )\n","print(\"Estimated expected cost misclassification using cost-sensitive decision tree: \"\n","          +  str(cost_CSDT) )\n","print(\"Ratio (o-t DT/CSDT): \" + str(np.round(100 * cost_DT_ot / cost_CSDT,1)) + \"%.\")"]},{"cell_type":"markdown","metadata":{"id":"5dnkksFnL1xJ"},"source":["##### Question\n","Also evaluate the costs on the validation set."]},{"cell_type":"markdown","metadata":{"id":"OFbGN7dcL1xJ"},"source":["##### Question\n","Build a cost-sensitive random forest and evaluate its performance. See https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier"]},{"cell_type":"markdown","metadata":{"id":"mWZjQ1EzL1xJ"},"source":["##### Question (optional)\n","You could also use a wrapper to determine the optimal specification of the class-weights."]},{"cell_type":"markdown","metadata":{"id":"3cfeeUgvL1xK"},"source":["## 4.3. Sampling methods - oversampling of minority class"]},{"cell_type":"markdown","metadata":{"id":"tHnaL3ziL1xK"},"source":["We will use the imbalanced-learn package. See https://imbalanced-learn.readthedocs.io/en/stable/index.html for the documentation."]},{"cell_type":"markdown","metadata":{"id":"rsiQsKwUL1xK"},"source":["We use the \"over-sampler\" from the imbalanced-learn package to obtain a balanced distribution of the target."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtbqNnRoL1xK"},"outputs":[],"source":["seed = 123\n","ros = RandomOverSampler(sampling_strategy=\"not majority\", random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irJWijQzL1xK"},"outputs":[],"source":["X_train_over, y_train_over = ros.fit_resample(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lVkDcVhL1xK"},"outputs":[],"source":["print(\"Distribution of target in train set:\")\n","print(pd.DataFrame(y_train).groupby(\"Class\", axis=0).size())\n","print(\"\\n\")\n","print(\"Distribution of target in oversampled train set:\")\n","print(pd.DataFrame(y_train_over).groupby(\"Class\", axis=0).size())"]},{"cell_type":"markdown","metadata":{"id":"6oeMkDQlL1xK"},"source":["##### Estimate decision tree on this rebalanced set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuEX2zzDL1xK"},"outputs":[],"source":["clf_over = tree.DecisionTreeClassifier()\n","dto = clf_over.fit(X_train_over, y_train_over)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fuzlaI7iL1xK"},"outputs":[],"source":["hat_y_train_dto = dto.predict(X_train_over)\n","hat_y_val_dto = dto.predict(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEwO8PZUL1xK"},"outputs":[],"source":["print(\"Results for standard decision tree validation set:\")\n","plot_confusion_matrix(hat_y_val_dt, y_val, target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykeeKaA2L1xK"},"outputs":[],"source":["print(\"Results for over-sampled decision tree on original validation set:\")\n","plot_confusion_matrix(hat_y_val_dto, y_val, target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lt4hU54tL1xL"},"outputs":[],"source":["cost_DT = estimate_cost(hat_y_val_dt, y_val, cost_FP, cost_FN)\n","cost_CSDT = estimate_cost(hat_y_val_ctree, y_val, cost_FP, cost_FN)\n","cost_dto = estimate_cost(hat_y_val_dto, y_val, cost_FP, cost_FN)\n","print(\"Ratio (DT/DTOver): \" + str(np.round(100 * cost_DT / cost_dto, 1)) + \"%.\")\n","print(\"Ratio (CSDT/DTOver): \" + str(np.round(100 * cost_CSDT / cost_dto, 1)) + \"%.\")"]},{"cell_type":"markdown","metadata":{"id":"Qi8AkyBML1xL"},"source":["### Question:\n","Analyze the performances."]},{"cell_type":"markdown","metadata":{"id":"ZLkVPMqcL1xL"},"source":["## 4.4. Sampling methods - SMOTE"]},{"cell_type":"markdown","metadata":{"id":"iia9M9w3L1xL"},"source":["The SMOTE function in Scikit-imbalanced learn uses the SMOTE to oversample\n","(the standard setting is to obtain a uniform class distribution)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IS17kpdhL1xL"},"outputs":[],"source":["X_train_SMOTE, y_train_SMOTE = SMOTE(random_state=38).fit_resample(X_train, y_train)\n","print(\"Distribution of target in train set:\")\n","print(pd.DataFrame(y_train).groupby(\"Class\", axis=0).size())\n","print(\"\\n\")\n","print(\"Distribution of target in oversampled train set:\")\n","print(pd.DataFrame(y_train_SMOTE).groupby(\"Class\", axis=0).size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goSDPsuAL1xL"},"outputs":[],"source":["clf_smote = tree.DecisionTreeClassifier()\n","clf_smote.fit(X_train_SMOTE, y_train_SMOTE)\n","hat_y_train_dt_SMOTE = clf_smote.predict(X_train_SMOTE)\n","hat_y_val_dt_SMOTE = clf_smote.predict(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbWwK0_QL1xL"},"outputs":[],"source":["cost_dt_smote = estimate_cost(hat_y_val_dt_SMOTE, y_val, cost_FP, cost_FN)\n","print(\"Ratio (DT/DTSMOTE): \" + str(np.round(100 * cost_DT / cost_dt_smote, 1)) + \"%\")\n","print(\"Ratio (CSDT/DTOver): \" + str(np.round(100 * cost_CSDT / cost_dto,1)) + \"%\")\n","print(\"Ratio (DTSMOTE/DTOver): \" + str(np.round(100 * cost_dt_smote / cost_dto,1)) + \"%\")"]},{"cell_type":"markdown","metadata":{"id":"F0v-P8WYL1xL"},"source":["### SMOTE with wrapper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzryYArFL1xL"},"outputs":[],"source":["n_fraud = sum(y_train)\n","n_nfraud = sum(1 - y_train)\n","factors = [1, 10, 100, 500]\n","for factor in factors:\n","    sm = SMOTE(sampling_strategy={0 : n_nfraud, 1 : int(n_fraud * factor)}, random_state=123)\n","    X_train_SMOTEaux, y_train_SMOTEaux = sm.fit_resample(X_train, y_train)\n","    DTSMOTEaux = tree.DecisionTreeClassifier(random_state=123)\n","    DTSMOTEaux.fit(X_train_SMOTEaux, y_train_SMOTEaux)\n","    hat_y_val_SMOTEaux = DTSMOTEaux.predict(X_val)\n","    cost_DTSMOTEaux = estimate_cost(hat_y_val_SMOTEaux, y_val, cost_FP, cost_FN)\n","    print(\"Oversampling Y=1, using SMOTE, by factor \" + str(int(factor)) + \", yields ratio (CSDT/DTSMOTE): \" + str(np.round(100 * cost_CSDT / cost_DTSMOTEaux,1)) + \"%\")"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"x74NGAjUL1xL"},"source":["### Question:\n","Determine your favourite SMOTE-factor."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"dwlfbBkXL1xM"},"source":["### Question:  \n","Choose your Top 3 of models (you are also allowed to estimate new ones)\n","and evaluate them on the test set."]},{"cell_type":"markdown","metadata":{"id":"f0oEZR6qL1xM"},"source":["# 5. Anomaly detection\n","\n","We will use the unsupervised learning algorithm isolation forest as an alternative to the supervised methods we analyzed above."]},{"cell_type":"markdown","metadata":{"id":"RP51LaD5L1xM"},"source":["First we determine the precision-recall curve for our random forest:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3wtRwTqL1xM"},"outputs":[],"source":["def draw_precision_recall(scores, y):\n","    precision, recall, thresholds = precision_recall_curve(y, scores)\n","    plt.fill_between(recall, precision, alpha=0.2, color=\"b\")\n","    plt.xlabel(\"Recall\")\n","    plt.ylabel(\"Precision\")\n","    plt.ylim([0.0, 1.05])\n","    plt.xlim([0.0, 1.0])\n","    plt.title(\"Precision-Recall curve\")\n","    plt.show()\n","draw_precision_recall(rf.predict_proba(X_val)[:, 1], y_val)"]},{"cell_type":"markdown","metadata":{"id":"zeRBPEyDL1xM"},"source":["#### Next, we consider the isolation forest."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ii1rXOlLL1xM"},"outputs":[],"source":["isof = IsolationForest(n_estimators=100)"]},{"cell_type":"markdown","metadata":{"id":"GFADH14aL1xM"},"source":["Train an isolation forest (note that y_train is not used):  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6zxiEbXL1xM"},"outputs":[],"source":["isof.fit(X_train)"]},{"cell_type":"markdown","metadata":{"id":"nGtub_SpL1xM"},"source":["Determine scores (related to depth) on validation set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ds6bFHuZL1xM"},"outputs":[],"source":["isof_scores_val = isof.decision_function(X_val)"]},{"cell_type":"markdown","metadata":{"id":"LlyNLxn-L1xN"},"source":["Evaluate distribution of scores in groups \"fraud\" and \"non-fraud\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BA-v0jLNL1xN"},"outputs":[],"source":["isof_scores_val_fraud = isof_scores_val[y_val==1]\n","isof_scores_val_nfraud = isof_scores_val[y_val==0]\n","left =min(np.nanmin(isof_scores_val_fraud), np.nanmin(isof_scores_val_nfraud))\n","right =max(np.nanmax(isof_scores_val_fraud), np.nanmax(isof_scores_val_nfraud))\n","plt.hist([isof_scores_val_fraud, isof_scores_val_nfraud], bins=25, range=[left, right], label=['1', '0'], density=True)\n","plt.legend(loc=\"upper right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HbaLT9S5L1xN"},"source":["Precision-recall curve of the isolation forest:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvM05k8vL1xN"},"outputs":[],"source":["draw_precision_recall(-1 * isof_scores_val, y_val)"]},{"cell_type":"markdown","metadata":{"id":"chKhCBOgL1xN"},"source":["##### Question\n","Consider the test set. Suppose that we are allowed to generate 150 alerts. Determine the alerts generated by the random forests and the alerts generated by the isolation forest.  Determine the precisions and the recalls."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ptVwNWsL1xN"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}